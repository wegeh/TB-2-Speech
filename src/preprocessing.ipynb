{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50bba2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f0227e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marvin Scifo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import subprocess\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497f2f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('transcripts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f69b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceID</th>\n",
       "      <th>Transcript</th>\n",
       "      <th>Device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>speaker01_m_nn_utt01</td>\n",
       "      <td>aku libur sedina merga ana banjir</td>\n",
       "      <td>Samsung M15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>speaker01_m_nn_utt02</td>\n",
       "      <td>aku pengin mangan dhawet nang omah</td>\n",
       "      <td>Samsung M15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>speaker01_m_nn_utt03</td>\n",
       "      <td>aku kara adhiku mangan sega goreng</td>\n",
       "      <td>Samsung M15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>speaker01_m_nn_utt04</td>\n",
       "      <td>aku tuku buku nang sekolahan</td>\n",
       "      <td>Samsung M15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>speaker01_m_nn_utt05</td>\n",
       "      <td>aku mung duwe panuwun supaya anakku pinter lan...</td>\n",
       "      <td>Samsung M15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>speaker70_m_nn_utt26</td>\n",
       "      <td>Tulung priksa apa kabeh data wis cocog.</td>\n",
       "      <td>Laptop Asus Vivobook 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>speaker70_m_nn_utt27</td>\n",
       "      <td>Kita lagi nglumpukake masukan saka para peserta.</td>\n",
       "      <td>Laptop Asus Vivobook 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>speaker70_m_nn_utt28</td>\n",
       "      <td>Aku bakal nerusake gaweanku sawise ngaso sak b...</td>\n",
       "      <td>Laptop Asus Vivobook 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>speaker70_m_nn_utt29</td>\n",
       "      <td>Aku lagi nyiapake jadwal kanggo dina iki.</td>\n",
       "      <td>Laptop Asus Vivobook 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>speaker70_m_nn_utt30</td>\n",
       "      <td>Esuk iki cuacane cerah lan anget.</td>\n",
       "      <td>Laptop Asus Vivobook 15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                SentenceID                                         Transcript  \\\n",
       "0     speaker01_m_nn_utt01                  aku libur sedina merga ana banjir   \n",
       "1     speaker01_m_nn_utt02                 aku pengin mangan dhawet nang omah   \n",
       "2     speaker01_m_nn_utt03                 aku kara adhiku mangan sega goreng   \n",
       "3     speaker01_m_nn_utt04                       aku tuku buku nang sekolahan   \n",
       "4     speaker01_m_nn_utt05  aku mung duwe panuwun supaya anakku pinter lan...   \n",
       "...                    ...                                                ...   \n",
       "2095  speaker70_m_nn_utt26            Tulung priksa apa kabeh data wis cocog.   \n",
       "2096  speaker70_m_nn_utt27   Kita lagi nglumpukake masukan saka para peserta.   \n",
       "2097  speaker70_m_nn_utt28  Aku bakal nerusake gaweanku sawise ngaso sak b...   \n",
       "2098  speaker70_m_nn_utt29          Aku lagi nyiapake jadwal kanggo dina iki.   \n",
       "2099  speaker70_m_nn_utt30                  Esuk iki cuacane cerah lan anget.   \n",
       "\n",
       "                       Device  \n",
       "0                 Samsung M15  \n",
       "1                 Samsung M15  \n",
       "2                 Samsung M15  \n",
       "3                 Samsung M15  \n",
       "4                 Samsung M15  \n",
       "...                       ...  \n",
       "2095  Laptop Asus Vivobook 15  \n",
       "2096  Laptop Asus Vivobook 15  \n",
       "2097  Laptop Asus Vivobook 15  \n",
       "2098  Laptop Asus Vivobook 15  \n",
       "2099  Laptop Asus Vivobook 15  \n",
       "\n",
       "[2100 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e879fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceID    2100\n",
      "Transcript    2088\n",
      "Device          47\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb7c7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Transcript'] = [i.lower() for i in df['Transcript']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b96a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.05,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e51d7afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('./../audio/audio_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da70c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    if f.endswith(\".wav\"):\n",
    "        in_file = f\"./../audio/audio_input/{f}\"\n",
    "        out_file = f\"./../audio/audio_waved/{f}\"\n",
    "\n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-y\",\n",
    "            \"-i\", in_file,\n",
    "            \"-ac\", \"1\",\n",
    "            \"-ar\", \"16000\",\n",
    "            \"-sample_fmt\", \"s16\",\n",
    "            out_file,\n",
    "        ]\n",
    "        subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    else:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af699cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    os.rename(f\"./../audio/audio_input/{files[i]}\", f\"./../audio/audio_input/{files[i].lower()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ec6df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('./../audio/audio_waved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc757d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    audio, sr = torchaudio.load(f'./../audio/audio_waved/{f}')\n",
    "\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio.mean(dim=0)\n",
    "        \n",
    "    audio = torchaudio.functional.resample(audio, sr, 16000)\n",
    "\n",
    "    sf.write(f'./../audio/audio_preprocessed/{f}', audio.squeeze().numpy(), 16000, subtype='PCM_16', format='WAV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0ff1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce0adcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"johaness14/wav2vec2-conformer-rope-jv-openslr\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForCTC.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdf6296f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ConformerForCTC(\n",
       "  (wav2vec2_conformer): Wav2Vec2ConformerModel(\n",
       "    (feature_extractor): Wav2Vec2ConformerFeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2ConformerLayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2ConformerLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2ConformerLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2ConformerFeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2ConformerEncoder(\n",
       "      (embed_positions): Wav2Vec2ConformerRotaryPositionalEmbedding()\n",
       "      (pos_conv_embed): Wav2Vec2ConformerPositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2ConformerSamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2ConformerEncoderLayer(\n",
       "          (ffn1_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn1): Wav2Vec2ConformerFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): SiLU()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (self_attn): Wav2Vec2ConformerSelfAttention(\n",
       "            (linear_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (linear_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (conv_module): Wav2Vec2ConformerConvolutionModule(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (pointwise_conv1): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (glu): GLU(dim=1)\n",
       "            (depthwise_conv): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=1024, bias=False)\n",
       "            (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): SiLU()\n",
       "            (pointwise_conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ffn2_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn2): Wav2Vec2ConformerFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): SiLU()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=29, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5c48aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx]\n",
    "\n",
    "        audio, sr = torchaudio.load(f\"./../audio/audio_preprocessed/{row['SentenceID']}.wav\")\n",
    "        audio = torchaudio.functional.resample(audio, sr, 16000)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            audio.squeeze().numpy(),\n",
    "            sampling_rate=16000,\n",
    "            text=row['Transcript'],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_values\": inputs[\"input_values\"].squeeze(0),\n",
    "            \"labels\": inputs[\"labels\"].squeeze(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4428c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ConformerDataset(train_df, processor)\n",
    "val_dataset   = ConformerDataset(val_df, processor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29cf7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "import torch\n",
    "\n",
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor, padding=True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(\n",
    "        self, \n",
    "        features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        # Extract inputs and labels\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        # Pad inputs\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Pad labels (separately)\n",
    "        labels_batch = self.processor.pad(\n",
    "            labels=label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Replace padding with -100 for the loss\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), \n",
    "            -100\n",
    "        )\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45b47e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding='longest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a088a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/750 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./conformer-test\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    dataloader_num_workers=0,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
